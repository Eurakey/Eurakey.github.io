<!DOCTYPE html>
<html lang='zh-CN'>

<head>
  <meta name="generator" content="Hexo 5.4.2">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.18.5">
  <meta charset="utf-8">
  

  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel='dns-prefetch' href='https://gcore.jsdelivr.net'>
  <link rel="preconnect" href="https://gcore.jsdelivr.net" crossorigin>
  <link rel='dns-prefetch' href='//unpkg.com'>

  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  
  <title>pytorch-learning - 一方净土</title>

  
    <meta name="description" content="pytorch学习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch-learning">
<meta property="og:url" content="https://eurakey.github.io/2023/03/21/pytorch-learning/">
<meta property="og:site_name" content="一方净土">
<meta property="og:description" content="pytorch学习笔记">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-03-21T07:13:19.000Z">
<meta property="article:modified_time" content="2023-03-21T07:14:17.242Z">
<meta property="article:author" content="Eureka">
<meta property="article:tag" content="python">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
  
  

  <!-- feed -->
  

  
    
<link rel="stylesheet" href="/css/main.css">

  

  
    <link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/Eurakey/picgodemo/img/rocket.png">
  

  

  


  
</head>

<body>
  




  <div class='l_body' id='start'>
    <aside class='l_left' layout='post'>
    

  

<header class="header"><div class="logo-wrap"><a class="avatar" href="/about/"><div class="bg" style="opacity:0;background-image:url(https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.4/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="https://cdn.jsdelivr.net/gh/Eurakey/picgodemo/imgavator.jpg" onerror="javascript:this.classList.add('error');this.src='null';"></a><a class="title" href="/%E4%B8%80%E6%96%B9%E5%87%80%E5%9C%9F"><div class="main" ff="title">一方净土</div><div class="sub normal cap">( •̀ ω •́ )y</div><div class="sub hover cap" style="opacity:0"> 😎😎😎</div></a></div>

<nav class="menu dis-select"><a class="nav-item active" href="/">文章</a><a class="nav-item" href="/wiki/">合集</a><a class="nav-item" href="/about/">关于</a></nav>
</header>


<div class="widgets">
<widget class="widget-wrapper search"><div class="widget-body"><div class="search-wrapper" id="search"><form class="search-form"><input type="text" class="search-input" id="search-input" data-filter="/blog/" placeholder="文章搜索"><svg t="1670596976048" class="icon search-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2676" width="200" height="200"><path d="M938.2 832.6L723.8 618.1c-2.5-2.5-5.3-4.4-7.9-6.4 36.2-55.6 57.3-121.8 57.3-193.1C773.3 222.8 614.6 64 418.7 64S64 222.8 64 418.6c0 195.9 158.8 354.6 354.6 354.6 71.3 0 137.5-21.2 193.2-57.4 2 2.7 3.9 5.4 6.3 7.8L832.5 938c14.6 14.6 33.7 21.9 52.8 21.9 19.1 0 38.2-7.3 52.8-21.8 29.2-29.1 29.2-76.4 0.1-105.5M418.7 661.3C284.9 661.3 176 552.4 176 418.6 176 284.9 284.9 176 418.7 176c133.8 0 242.6 108.9 242.6 242.7 0 133.7-108.9 242.6-242.6 242.6" p-id="2677"></path></svg></form><div id="search-result"></div><div class="search-no-result">没有找到内容！</div></div></div></widget>


<widget class="widget-wrapper toc single" id="data-toc"><div class="widget-header cap dis-select"><span class="name">pytorch-learning</span></div><div class="widget-body fs14"><div class="doc-tree active"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%A4%E5%A4%A7%E6%B3%95%E5%AE%9D%E5%87%BD%E6%95%B0"><span class="toc-text">两大法宝函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-text">如何加载数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TensorBoard"><span class="toc-text">TensorBoard</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transforms%E7%BB%93%E6%9E%84%E5%8F%8A%E7%94%A8%E6%B3%95"><span class="toc-text">Transforms结构及用法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84Transforms"><span class="toc-text">常见的Transforms</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E5%85%B7%E9%A3%9F%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-text">工具食用方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torchvision%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%BF%E7%94%A8"><span class="toc-text">torchvision中的数据集使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataLoader"><span class="toc-text">DataLoader</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%AA%A8%E6%9E%B6"><span class="toc-text">神经网络的基本骨架</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C"><span class="toc-text">卷积操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-text">神经网络-卷积层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%9C%80%E5%A4%A7%E5%8C%96%E6%B1%A0%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text">神经网络-最大化池的使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB"><span class="toc-text">神经网络-非线性激活</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%BA%BF%E6%80%A7%E5%B1%82%E5%8F%8A%E5%85%B6%E4%BB%96%E5%B1%82%E4%BB%8B%E7%BB%8D"><span class="toc-text">神经网络-线性层及其他层介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sequential%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text">Sequential的使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">损失函数与反向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%B0%E6%9C%89%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E4%BF%AE%E6%94%B9"><span class="toc-text">现有网络模型的使用及修改</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96"><span class="toc-text">网络模型的保存与读取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%A5%97%E8%B7%AF"><span class="toc-text">完整的模型训练套路</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%A9%E7%94%A8GPU%E8%AE%AD%E7%BB%83"><span class="toc-text">利用GPU训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81%E5%A5%97%E8%B7%AF-%E6%B5%8B%E8%AF%95%EF%BC%8C-demo"><span class="toc-text">完整的模型验证套路(测试， demo)</span></a></li></ol></div></div></widget>




</div>
<footer class="footer dis-select"><div class="social-wrap"><a class="social" href="https://github.com/Eurakey" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.4/social/08a41b181ce68.svg"/></a></div></footer>

    </aside>
    <div class='l_main'>
      

      


<div class="bread-nav fs12"><div id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a><span class="sep"></span><a class="cap breadcrumb" href="/">文章</a></div><div id="post-meta">发布于&nbsp;<time datetime="2023-03-21T07:13:19.000Z">2023-03-21</time></div></div>

<article class='md-text content post'>
<h1 class="article-title"><span>pytorch-learning</span></h1>
<h2 id="两大法宝函数"><a href="#两大法宝函数" class="headerlink" title="两大法宝函数"></a>两大法宝函数</h2><ul>
<li>dir()：打开、看见</li>
<li>help():说明书<h2 id="如何加载数据"><a href="#如何加载数据" class="headerlink" title="如何加载数据"></a>如何加载数据</h2></li>
<li>Dataset</li>
<li>Dataloader<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset  </span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image  <span class="comment"># 操作图片</span></span><br><span class="line"><span class="keyword">import</span> os  <span class="comment"># 操作系统</span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, label_dir</span>):  </span><br><span class="line">        self.root_dir = root_dir  </span><br><span class="line">        self.label_dir = label_dir  </span><br><span class="line">        self.path = os.path.join(root_dir, label_dir)  </span><br><span class="line">        self.img_path = os.listdir(self.path)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):  <span class="comment"># 内置函数</span></span><br><span class="line">        img_name = self.img_path[idx]  </span><br><span class="line">        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)  </span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_item_path)  </span><br><span class="line">        label = self.label_dir  </span><br><span class="line">        <span class="keyword">return</span> img, label  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_path)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">root_dir = <span class="string">&quot;dataset/train&quot;</span>  </span><br><span class="line">ants_label_dir = <span class="string">&quot;ants_image&quot;</span>  </span><br><span class="line">ants_dataset = MyData(root_dir, ants_label_dir)</span><br></pre></td></tr></table></figure>
<h2 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter  </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image  </span><br><span class="line">  </span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)  </span><br><span class="line">image_path = <span class="string">&quot;dataset/train/ants_image/0013035.jpg&quot;</span>  </span><br><span class="line">img_PIL = Image.<span class="built_in">open</span>(image_path)  </span><br><span class="line">img_array = np.array(img_PIL)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(img_array.shape)  </span><br><span class="line">  </span><br><span class="line">writer.add_image(<span class="string">&quot;test&quot;</span>, img_array, <span class="number">1</span>, dataformats=<span class="string">&quot;HWC&quot;</span>)  </span><br><span class="line"><span class="comment"># writer.add_scalar()  </span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  </span><br><span class="line">    writer.add_scalar(<span class="string">&quot;y=2x&quot;</span>, <span class="number">2</span>*i, i)  </span><br><span class="line">  </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h2 id="Transforms结构及用法"><a href="#Transforms结构及用法" class="headerlink" title="Transforms结构及用法"></a>Transforms结构及用法</h2>transforms.py 工具箱</li>
<li>totensor</li>
<li>resize<br>通过 transforms.ToTensor去看两个问题</li>
</ul>
<ol>
<li>transforms该如何使用(python)</li>
<li>为什么我们需要Tensor数据类型<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter  </span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms  </span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image  </span><br><span class="line">  </span><br><span class="line">img_path = <span class="string">&quot;dataset/train/ants_image/0013035.jpg&quot;</span>  </span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)  </span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">tensor_trans = transforms.ToTensor() <span class="comment"># 创建工具  </span></span><br><span class="line">tensor_img = tensor_trans(img) <span class="comment"># 使用工具  </span></span><br><span class="line">  </span><br><span class="line">writer.add_image(<span class="string">&quot;Tensor_img&quot;</span>, tensor_img)  </span><br><span class="line">  </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h2 id="常见的Transforms"><a href="#常见的Transforms" class="headerlink" title="常见的Transforms"></a>常见的Transforms</h2></li>
</ol>
<ul>
<li>输入 PIL Image.open()</li>
<li>输出 tensor ToTensor()</li>
<li>作用 narays cv.imread()<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image  </span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms  </span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter  </span><br><span class="line">  </span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)  </span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&quot;dataset/train/ants_image/0013035.jpg&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#ToTensor  </span></span><br><span class="line">trans_tensor = transforms.ToTensor()  </span><br><span class="line">img_tensor = trans_tensor(img)  </span><br><span class="line">  </span><br><span class="line">writer.add_image(<span class="string">&quot;ToTensor&quot;</span>, img_tensor)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># Normalize  </span></span><br><span class="line"><span class="built_in">print</span>(img_tensor[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>])  </span><br><span class="line">trans_norm = transforms.Normalize([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])  </span><br><span class="line">img_norm = trans_norm(img_tensor)  </span><br><span class="line"><span class="built_in">print</span>(img_norm[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>])  </span><br><span class="line">writer.add_image(<span class="string">&quot;Normalize&quot;</span>, img_norm, <span class="number">1</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># Resize  </span></span><br><span class="line"><span class="built_in">print</span>(img.size)  </span><br><span class="line">trans_resize = transforms.Resize((<span class="number">512</span>, <span class="number">512</span>))  </span><br><span class="line">img_resize = trans_resize(img)  </span><br><span class="line"><span class="built_in">print</span>(img_resize.size)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># Compose  </span></span><br><span class="line">trans_compose = transforms.Compose([trans_resize, trans_tensor])  </span><br><span class="line">img_compose = trans_compose(img)  </span><br><span class="line">writer.add_image(<span class="string">&quot;compose&quot;</span>, img_compose, <span class="number">1</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># RandomCrop  </span></span><br><span class="line">trans_random = transforms.RandomCrop((<span class="number">500</span>, <span class="number">1000</span>))  </span><br><span class="line">trans_compose_2 = transforms.Compose([trans_random, trans_tensor])  </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  </span><br><span class="line">    img_crop = trans_compose_2(img)  </span><br><span class="line">    writer.add_image(<span class="string">&quot;RandomCrop&quot;</span>, img_crop, i)  </span><br><span class="line">  </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h3 id="工具食用方法"><a href="#工具食用方法" class="headerlink" title="工具食用方法"></a>工具食用方法</h3>关注输入输出，官方文档<h2 id="torchvision中的数据集使用"><a href="#torchvision中的数据集使用" class="headerlink" title="torchvision中的数据集使用"></a>torchvision中的数据集使用</h2>在PyTorch中，<code>train=True</code>表示使用CIFAR-10数据集的训练集，而<code>train=False</code>表示使用测试集。在训练模型时，我们使用训练集来训练模型，然后使用测试集来评估模型的性能。因此，我们需要将数据集分成训练集和测试集.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision  </span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image  </span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter  </span><br><span class="line">  </span><br><span class="line">dataset_transform = torchvision.transforms.Compose([  </span><br><span class="line">    torchvision.transforms.ToTensor()  </span><br><span class="line">])  </span><br><span class="line">  </span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, transform=dataset_transform, download=<span class="literal">True</span>)  </span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, transform=dataset_transform, download=<span class="literal">True</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(test_set.classes)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># img, target = test_set[0]  </span></span><br><span class="line"><span class="comment"># print(img)  </span></span><br><span class="line"><span class="comment"># print(target)  </span></span><br><span class="line"><span class="comment"># print(test_set.classes[target])  </span></span><br><span class="line"><span class="comment"># img.show()  </span></span><br><span class="line"><span class="comment"># print(test_set[0])  </span></span><br><span class="line">write = SummaryWriter(<span class="string">&quot;p10&quot;</span>)  </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  </span><br><span class="line">    img, target = test_set[i]  </span><br><span class="line">    write.add_image(<span class="string">&quot;test_set&quot;</span>, img, i)  </span><br><span class="line">  </span><br><span class="line">write.close()</span><br></pre></td></tr></table></figure>
<h2 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h2></li>
<li>  <code>dataset</code>：要从中加载数据的数据集。</li>
<li>  <code>batch_size</code>：每个批次中的样本数。</li>
<li>  <code>shuffle</code>：是否对数据进行洗牌。</li>
<li>  <code>num_workers</code>：用于数据加载的子进程数。</li>
<li>  <code>drop_last</code>：如果数据集大小不能被批次大小整除，则是否删除最后一个不完整的批次<h2 id="神经网络的基本骨架"><a href="#神经网络的基本骨架" class="headerlink" title="神经网络的基本骨架"></a>神经网络的基本骨架</h2><code>nn.Module</code>是PyTorch中所有神经网络模块的基类。它包含层和一个方法<code>forward(input)</code>，该方法返回输出。它是一种方便的封装参数的方法，具有将它们移动到GPU、导出、加载等的帮助程序。您的模型也应该继承这个类。模块也可以包含其他模块，允许将它们嵌套在树形结构中。您可以将子模块分配为常规属性。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>(Tudui, self).__init__()  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):  </span><br><span class="line">        output = <span class="built_in">input</span> + <span class="number">1</span>  </span><br><span class="line">        <span class="keyword">return</span> output  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">tudui = Tudui()  </span><br><span class="line">x = torch.tensor(<span class="number">1.0</span>)  </span><br><span class="line">output = tudui(x)  </span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>
<h2 id="卷积操作"><a href="#卷积操作" class="headerlink" title="卷积操作"></a>卷积操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>],  </span><br><span class="line">                      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>],  </span><br><span class="line">                      [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],  </span><br><span class="line">                      [<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>],  </span><br><span class="line">                      [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]])  </span><br><span class="line">  </span><br><span class="line">kernel = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>],  </span><br><span class="line">                      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],  </span><br><span class="line">                      [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]])  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))  <span class="comment"># 增加batchsize和channel  </span></span><br><span class="line">kernel = torch.reshape(kernel, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>))  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)  </span><br><span class="line"><span class="built_in">print</span>(kernel.shape)  </span><br><span class="line">  </span><br><span class="line">output = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">1</span>)  </span><br><span class="line">output2 = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">2</span>)  </span><br><span class="line">output3 = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(output)  </span><br><span class="line"><span class="built_in">print</span>(output2)  </span><br><span class="line"><span class="built_in">print</span>(output3)</span><br></pre></td></tr></table></figure>
torch.nn.Conv2d是一个类，它可以创建一个二维卷积层，而torch.nn.functional.conv2d是一个函数，它可以对一个输入图像进行二维卷积操作。</li>
</ul>
<p>你的代码中有三个conv2d的调用，它们的区别是：</p>
<ul>
<li>output = F.conv2d(input, kernel, stride=1)：这个调用没有使用padding参数，所以输出的形状是(1, 1, 3, 3)。</li>
<li>output2 = F.conv2d(input, kernel, stride=2)：这个调用使用了stride参数为2，所以输出的形状是(1, 1, 2, 2)。</li>
<li>output3 = F.conv2d(input, kernel, stride=1, padding=1)：这个调用使用了padding参数为1，所以输出的形状是(1, 1, 5, 5)。</li>
</ul>
<h2 id="神经网络-卷积层"><a href="#神经网络-卷积层" class="headerlink" title="神经网络-卷积层"></a>神经网络-卷积层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">import</span> torchvision  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d  </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  </span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter  </span><br><span class="line">  </span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;data&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)  </span><br><span class="line">  </span><br><span class="line">dataloader = DataLoader(dataset, <span class="number">64</span>, shuffle=<span class="literal">True</span>)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>(Tudui, self).__init__()  </span><br><span class="line">        self.conv1 = Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size= <span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  </span><br><span class="line">        x = self.conv1(x)  </span><br><span class="line">        <span class="keyword">return</span> x  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">tudui = Tudui()  </span><br><span class="line">  </span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;p11&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">step = <span class="number">0</span>  </span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:  </span><br><span class="line">    imgs, targets =  data  </span><br><span class="line">    output = tudui(imgs)  </span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>, imgs, step)  </span><br><span class="line">  </span><br><span class="line">    output = torch.reshape(output, (-<span class="number">1</span>, <span class="number">3</span>, <span class="number">30</span>, <span class="number">30</span>))  </span><br><span class="line">  </span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>, output, step)  </span><br><span class="line">  </span><br><span class="line">    step += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>代码中有两个部分，一个是数据集和数据加载器的定义，另一个是自定义卷积层类Tudui的定义和使用。</p>
<ul>
<li>  数据集和数据加载器的定义：这里你使用了torchvision.datasets.CIFAR10来下载并加载CIFAR10数据集，它包含了60000张32x32的彩色图像，分为10个类别。你使用了torchvision.transforms.ToTensor()来把图像转换为张量，并且只加载了测试集。你使用了torch.utils.data.DataLoader来创建一个数据加载器，它可以按照批次大小为64，随机打乱顺序地提供数据。</li>
<li>  自定义卷积层类Tudui的定义和使用：这里你继承了torch.nn.Module类来创建一个自定义卷积层类Tudui。在__init__方法中，你使用了torch.nn.Conv2d来创建一个二维卷积层，它可以把输入通道数为3的图像转换为输出通道数为6的特征图，使用了3x3的卷积核，步长为1，没有填充。在forward方法中，你把输入x通过self.conv1进行卷积操作，并返回结果。然后你创建了一个Tudui的实例tudui，并用它对dataloader中的每一批图像进行处理。你使用了torch.utils.tensorboard.SummaryWriter来创建一个写入器writer，并用它把输入图像和输出特征图写入到TensorBoard中。<h2 id="神经网络-最大化池的使用"><a href="#神经网络-最大化池的使用" class="headerlink" title="神经网络-最大化池的使用"></a>神经网络-最大化池的使用</h2>最大池化（Max Pooling）是一种常用的池化操作，它可以减少特征图的大小，降低计算量和内存消耗，同时保留最重要的特征。</li>
<li>  kernel_size：滤波器的大小，也就是每次取最大值的窗口大小。可以是一个整数，表示窗口是正方形的，也可以是一个元组，表示窗口是矩形的。</li>
<li>  stride：滤波器的步长，也就是每次移动的距离。默认值是kernel_size，表示没有重叠。可以是一个整数，表示水平和垂直方向上都相同，也可以是一个元组，表示水平和垂直方向上不同。</li>
<li>  padding：在输入特征图两边添加的填充值。默认值是0，表示不添加填充。可以是一个整数，表示两边都相同，也可以是一个元组，表示两边不同。填充值为负无穷大（-inf），表示被忽略。</li>
<li>  dilation：滤波器中元素之间的间隔。默认值是1，表示没有间隔。可以是一个整数，表示水平和垂直方向上都相同，也可以是一个元组，表示水平和垂直方向上不同。</li>
<li>  return_indices：是否返回最大值对应的索引。默认值是False，表示不返回。如果为True，则返回一个与输出形状相同的张量（tensor），包含每个最大值在输入特征图中的位置（行列号）。这个参数在后续使用torch.nn.MaxUnpool2d进行反池化时有用。</li>
<li>  ceil_mode：是否使用向上取整而不是向下取整来计算输出形状。默认值是False，表示使用向下取整。如果为True，则使用向上取整。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">import</span> torchvision  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MaxPool2d  </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  </span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter  </span><br><span class="line">  </span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;data&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor())  </span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>(Tudui, self).__init__()  </span><br><span class="line">        self.maxpool1 = MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">True</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):  </span><br><span class="line">        output = self.maxpool1(<span class="built_in">input</span>)  </span><br><span class="line">        <span class="keyword">return</span> output  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">tudui = Tudui()  </span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs_maxpool&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">step = <span class="number">0</span>  </span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:  </span><br><span class="line">    imgs, targets = data  </span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>, imgs, step)  </span><br><span class="line">    output = tudui(imgs)  </span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>, output, step )  </span><br><span class="line">  </span><br><span class="line">    step = step + <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h2 id="神经网络-非线性激活"><a href="#神经网络-非线性激活" class="headerlink" title="神经网络-非线性激活"></a>神经网络-非线性激活</h2>非线性激活函数是指在神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。非线性激活函数对于神经网络去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到网络中。</li>
</ul>
<p>如果我们的神经网络只使用线性激活函数，则无论多深的网络最终输出也不过是所以输入的简单线性组合，这并不具有拟合任意函数的能力，因此我们需要引入非线性激活函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> ReLU  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>, -<span class="number">0.5</span>],  </span><br><span class="line">                      [-<span class="number">1</span>, <span class="number">3</span>]])  </span><br><span class="line">torch.reshape(<span class="built_in">input</span>, (-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>))  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TuDui</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>(TuDui, self).__init__()  </span><br><span class="line">        self.relu1 = ReLU()  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):  </span><br><span class="line">        output = self.relu1(<span class="built_in">input</span>)  </span><br><span class="line">        <span class="keyword">return</span> output  </span><br><span class="line">  </span><br><span class="line">tudui = TuDui()  </span><br><span class="line">output = tudui(<span class="built_in">input</span>)  </span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>
<h2 id="神经网络-线性层及其他层介绍"><a href="#神经网络-线性层及其他层介绍" class="headerlink" title="神经网络-线性层及其他层介绍"></a>神经网络-线性层及其他层介绍</h2><p>线性层是一种神经网络中的基本模块，它使用一个权重矩阵对输入特征进行线性变换，得到输出特征。线性层的输入和输出都是二维张量，形状为[batch_size, size]。线性层可以用于实现全连接层，即每个输入神经元与每个输出神经元都有连接。</p>
<p>线性层的参数有：</p>
<ul>
<li>  in_features：输入特征的数量，即输入张量的列数</li>
<li>  out_features：输出特征的数量，即输出张量的列数</li>
<li>  bias：是否添加偏置项，默认为True<h2 id="Sequential的使用"><a href="#Sequential的使用" class="headerlink" title="Sequential的使用"></a>Sequential的使用</h2>它有三个卷积层，三个最大池化层，两个全连接层和一个Flatten层。它的输入是一个64x3x32x32的张量，输出是一个64x10的张量。你还使用了SummaryWriter来可视化你的模型结构。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential  </span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TuDui</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>(TuDui, self).__init__()  </span><br><span class="line">        self.model1 = Sequential(  </span><br><span class="line">            Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),  </span><br><span class="line">            MaxPool2d(<span class="number">2</span>),  </span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),  </span><br><span class="line">            MaxPool2d(<span class="number">2</span>),  </span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),  </span><br><span class="line">            MaxPool2d(<span class="number">2</span>),  </span><br><span class="line">            Flatten(),  </span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),  </span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)  </span><br><span class="line">        )  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  </span><br><span class="line">        x = self.model1(x)  </span><br><span class="line">  </span><br><span class="line">        <span class="keyword">return</span> x  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">tudui = TuDui()  </span><br><span class="line"><span class="built_in">print</span>(tudui)  </span><br><span class="line"><span class="built_in">input</span> = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))  </span><br><span class="line">output = tudui(<span class="built_in">input</span>)  </span><br><span class="line"><span class="built_in">print</span>(output.shape)  </span><br><span class="line">  </span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs_seq&quot;</span>)  </span><br><span class="line">writer.add_graph(tudui, <span class="built_in">input</span>)  </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h2 id="损失函数与反向传播"><a href="#损失函数与反向传播" class="headerlink" title="损失函数与反向传播"></a>损失函数与反向传播</h2>神经网络的损失函数是用来衡量网络输出与期望输出之间的差异，反向传播是一种算法，用来计算损失函数对网络参数的梯度，并根据梯度更新参数，使损失函数达到最小<br>神经网络的梯度是一个重要的概念，它指示了在函数的某一点，沿着哪个方向函数值上升最快，以及上升的速度有多快<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">import</span> torchvision  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential, CrossEntropyLoss  </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  </span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter  </span><br><span class="line">  </span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;data&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)  </span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">1</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TuDui</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>(TuDui, self).__init__()  </span><br><span class="line">        self.model1 = Sequential(  </span><br><span class="line">            Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),  </span><br><span class="line">            MaxPool2d(<span class="number">2</span>),  </span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),  </span><br><span class="line">            MaxPool2d(<span class="number">2</span>),  </span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),  </span><br><span class="line">            MaxPool2d(<span class="number">2</span>),  </span><br><span class="line">            Flatten(),  </span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),  </span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)  </span><br><span class="line">        )  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  </span><br><span class="line">        x = self.model1(x)  </span><br><span class="line">  </span><br><span class="line">        <span class="keyword">return</span> x  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">tudui = TuDui()  </span><br><span class="line">loss = CrossEntropyLoss()  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:  </span><br><span class="line">    imgs, targets = data  </span><br><span class="line">    outputs = tudui(imgs)  </span><br><span class="line">    result_loss = loss(outputs, targets)  </span><br><span class="line">    <span class="built_in">print</span>(result_loss)  </span><br><span class="line">    result_loss.backward()</span><br></pre></td></tr></table></figure>
<h2 id="现有网络模型的使用及修改"><a href="#现有网络模型的使用及修改" class="headerlink" title="现有网络模型的使用及修改"></a>现有网络模型的使用及修改</h2>如果你想使用预训练模型，你可以参考 Pytorch 的官方文档或者 Pytorch Hub，那里有很多现成的模型和示例代码。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/xgxg1314/article/details/111187424">如果你想自定义模型，你需要继承 torch.nn.Module 类，并实现 <strong>init</strong> 和 forward 方法。<strong>init</strong> 方法用于定义模型的层和参数，forward 方法用于定义模型的前向传播逻辑。</a><a target="_blank" rel="noopener" href="https://blog.csdn.net/xgxg1314/article/details/111187424">1</a><a target="_blank" rel="noopener" href="https://blog.csdn.net/andyL_05/article/details/103363603">2</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/82038049">无论是使用预训练模型还是自定义模型，你都需要保存和加载模型的状态字典（state_dict），这是一个包含了每个网络层和其对应参数张量的 Python 字典。</a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/82038049">3</a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/73893187">4</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/82038049">你可以使用 torch.save 和 torch.load 函数来保存和加载状态字典。</a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/82038049">3</a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/73893187">4</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/73893187">如果你想在不同的设备上保存和加载模型（比如 CPU 和 GPU），你需要注意指定正确的设备信息。</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_45772756/article/details/128709362">【学习笔记】【Pytorch】十四、现有网络模型的使用及修改_pytorch 修改网络结构_Mr庞.的博客-CSDN博客</a></p>
<h2 id="网络模型的保存与读取"><a href="#网络模型的保存与读取" class="headerlink" title="网络模型的保存与读取"></a>网络模型的保存与读取</h2><p>网络模型的保存与读取是一个常见的需求，它可以帮助我们在不同的设备或场景下使用已经训练好的模型。</p>
<p>不同的深度学习框架有不同的方法来保存和读取网络模型，这里我以 Pytorch 为例，给你简单介绍一下。</p>
<p>Pytorch 提供了两种保存和读取网络模型的方式：</p>
<ul>
<li>  <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38600065/article/details/106969037">第一种是保存整个网络模型，包括模型结构和参数。这种方式可以直接使用 torch.save(model, ‘model.pth’) 和 model = torch.load(‘model.pth’) 函数来实现。</a><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38600065/article/details/106969037">1</a></li>
<li>  <a target="_blank" rel="noopener" href="https://blog.csdn.net/lien0906/article/details/110481970">第二种是只保存网络参数，也就是状态字典（state_dict）。这种方式可以使用 torch.save(model.state_dict(), ‘model_params.pth’) 和 model.load_state_dict(torch.load(‘model_params.pth’)) 函数来实现。</a><a target="_blank" rel="noopener" href="https://blog.csdn.net/lien0906/article/details/110481970">2</a><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38600065/article/details/106969037">1</a></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lien0906/article/details/110481970">Pytorch 官方推荐使用第二种方式，因为它更灵活和通用，而且占用的空间更小。</a><a target="_blank" rel="noopener" href="https://blog.csdn.net/lien0906/article/details/110481970">2</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lien0906/article/details/110481970">无论哪种方式，你都需要注意在不同设备上保存和加载模型时指定正确的设备信息（比如 CPU 或 GPU）。</a><a target="_blank" rel="noopener" href="https://blog.csdn.net/lien0906/article/details/110481970">2</a><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38600065/article/details/106969037">1</a></p>
<h2 id="完整的模型训练套路"><a href="#完整的模型训练套路" class="headerlink" title="完整的模型训练套路"></a>完整的模型训练套路</h2><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Arg_max">argmax 是一个数学函数，它用于找到使目标函数取得最大值的输入（或参数）。</a><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Arg_max">1</a></p>
<p><a target="_blank" rel="noopener" href="https://machinelearningmastery.com/argmax-in-machine-learning/">在机器学习中，argmax 常用于找到具有最大预测概率的类别。</a><a target="_blank" rel="noopener" href="https://machinelearningmastery.com/argmax-in-machine-learning/">2</a></p>
<p>例如，如果有一个分类器可以对一张图片进行四种类别的预测（猫、狗、鸟、鱼），并输出每种类别的概率（0.2、0.3、0.4、0.1），那么 argmax 操作可以返回最大概率对应的类别（鸟）或者索引（2）。</p>
<p><a target="_blank" rel="noopener" href="https://numpy.org/doc/stable/reference/generated/numpy.argmax.html">在 Python 中，可以使用 numpy.argmax() 函数来实现 argmax 操作。</a><a target="_blank" rel="noopener" href="https://numpy.org/doc/stable/reference/generated/numpy.argmax.html">3</a><a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/numpy-argmax-python/">4</a></p>
<p>这个函数接受一个数组作为输入，并返回沿着指定轴的最大值的索引。如果没有指定轴，则返回整个数组中的最大值的索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">import</span> torchvision  </span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> Tudui  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 准备数据集  </span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  </span><br><span class="line">  </span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&quot;data&quot;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)  </span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;data&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 长度  </span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)  </span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试数据集的长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 利用DataLoader来加载数据  </span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=<span class="number">64</span>)  </span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 创建网络模型  </span></span><br><span class="line">tudui = Tudui()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 损失函数  </span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 优化器  </span></span><br><span class="line">learing_rate = <span class="number">1e-2</span>  </span><br><span class="line">ooptimizer = torch.optim.SGD(tudui.parameters(), lr=learing_rate)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 设置训练网络的一些参数  </span></span><br><span class="line"><span class="comment"># 记录训练次数  </span></span><br><span class="line">total_train_step = <span class="number">0</span>  </span><br><span class="line"><span class="comment"># 记录测试的次数  </span></span><br><span class="line">total_test_step = <span class="number">0</span>  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#训练的轮数  </span></span><br><span class="line">epoch = <span class="number">10</span>  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 添加tensorboard  </span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs_train&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----------------------------第&#123;&#125;轮训练开始----------------------------------&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 训练步骤开始  </span></span><br><span class="line">    tudui.train()  </span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:  </span><br><span class="line">        imgs, targets = data  </span><br><span class="line">        outputs = tudui(imgs)  </span><br><span class="line">        loss = loss_fn(outputs, targets)  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 优化器优化模型  </span></span><br><span class="line">        ooptimizer.zero_grad()  </span><br><span class="line">        loss.backward()  </span><br><span class="line">        ooptimizer.step()  </span><br><span class="line">  </span><br><span class="line">        total_train_step = total_train_step + <span class="number">1</span>  </span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:  </span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数：&#123;&#125;, Loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))  </span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>, loss.item(), total_train_step)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 测试步骤开始  </span></span><br><span class="line">    tudui.<span class="built_in">eval</span>()  </span><br><span class="line">    total_test_loss = <span class="number">0</span>  </span><br><span class="line">    total_accuracy = <span class="number">0</span>  </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  </span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:  </span><br><span class="line">            imgs, targets = data  </span><br><span class="line">            outputs = tudui(imgs)  </span><br><span class="line">            loss = loss_fn(outputs, targets)  </span><br><span class="line">            total_test_loss = total_test_loss + loss.item()  </span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()  </span><br><span class="line">            total_accuracy = total_accuracy + accuracy  </span><br><span class="line">  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy/ test_data_size))  </span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>, total_test_loss, total_test_step)  </span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>, total_accuracy/test_data_size, total_test_step)  </span><br><span class="line">    total_test_step = total_test_step + <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">    torch.save(tudui, <span class="string">&quot;tudui_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i))  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型已保存&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<h2 id="利用GPU训练"><a href="#利用GPU训练" class="headerlink" title="利用GPU训练"></a>利用GPU训练</h2><pre><code>1. 找到
- 网络模型
- 数据（输入输出）
- 损失函数
.cuda()
2. .to(device)
device = torch.device(&quot;cpu&quot;)
Torch.device(&quot;cuda&quot;)
</code></pre>
<h2 id="完整的模型验证套路-测试，-demo"><a href="#完整的模型验证套路-测试，-demo" class="headerlink" title="完整的模型验证套路(测试， demo)"></a>完整的模型验证套路(测试， demo)</h2><p>利用已经训练好的模型，然后给它提供输入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">import</span> torchvision  </span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 传入图片处理  </span></span><br><span class="line">img_path = <span class="string">&quot;./imgs/dog.jpg&quot;</span>  </span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)  </span><br><span class="line">img = img.convert(<span class="string">&quot;RGB&quot;</span>)  </span><br><span class="line">transform = torchvision.transforms.Compose([  </span><br><span class="line">    torchvision.transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),  </span><br><span class="line">    torchvision.transforms.ToTensor()  </span><br><span class="line">])  </span><br><span class="line">img = transform(img)  </span><br><span class="line">img = torch.reshape(img, (<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))  </span><br><span class="line">img = img.cuda()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 开始识别  </span></span><br><span class="line">model = torch.load(<span class="string">&quot;tudui_0.pth&quot;</span>)  </span><br><span class="line">model.cuda()  </span><br><span class="line">model.<span class="built_in">eval</span>()  </span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  </span><br><span class="line">    output = model(img)  </span><br><span class="line"><span class="built_in">print</span>(output.argmax(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<div class="article-footer reveal fs14"><section id="license"><div class="header"><span>许可协议</span></div><div class="body"><p>本文采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 4.0 国际</a> 许可协议，转载请注明出处。</p>
</div></section></div>

</article>

<div class="related-wrap reveal" id="read-next"><section class="body"><div class="item" id="prev"></div><div class="item" id="next"><div class="note">较早文章</div><a href="/2023/02/23/gpg/">GPG简要入门指南</a></div></section></div>






  <div class='related-wrap md-text reveal' id="comments">
    <section class='header cmt-title cap theme'>
      快来参与讨论吧
    </section>
    <section class='body cmt-body giscus'>
      

<svg class="loading" style="vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2709"><path d="M832 512c0-176-144-320-320-320V128c211.2 0 384 172.8 384 384h-64zM192 512c0 176 144 320 320 320v64C300.8 896 128 723.2 128 512h64z" p-id="2710"></path></svg>

<div id="giscus" data-repo="Eurakey/Eurakey.github.io" data-repo-id="R_kgDOIx0isA" data-category="Announcements" data-category-id="DIC_kwDOIx0isM4CTn49" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="preferred_color_scheme" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous"></div>

    </section>
  </div>



      
<footer class="page-footer reveal fs12"><hr><div class="text"><p>本站由 <a target="_blank" rel="noopener" href="https://github.com/Eurakey">@Eureka</a> 使用 <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar">Stellar</a> 主题创建。<br>本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处。</p>
</div></footer>

      <div class='float-panel mobile-only blur' style='display:none'>
  <button type='button' class='sidebar-toggle mobile' onclick='sidebar.toggle()'>
    <svg class="icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15301"><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 2.3 26.8 24.6 47.5 51.6 47.6h416.5v4z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15302"></path><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 1.9 27.7 23.9 49.7 51.6 51.6h416.5z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15303"></path></svg>
  </button>
</div>

    </div>
  </div>
  <div class='scripts'>
    <script type="text/javascript">
  const stellar = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    loadCSS: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    // 从 butterfly 和 volantis 获得灵感
    loadScript: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    // https://github.com/jerryc127/hexo-theme-butterfly
    jQuery: (fn) => {
      if (typeof jQuery === 'undefined') {
        stellar.loadScript(stellar.plugins.jQuery).then(fn)
      } else {
        fn()
      }
    }
  };
  stellar.version = '1.18.5';
  stellar.github = 'https://github.com/xaoxuu/hexo-theme-stellar/tree/1.18.5';
  stellar.config = {
    date_suffix: {
      just: '刚刚',
      min: '分钟前',
      hour: '小时前',
      day: '天前',
      month: '个月前',
    },
  };

  // required plugins (only load if needs)
  stellar.plugins = {
    jQuery: 'https://gcore.jsdelivr.net/npm/jquery@3.6.2/dist/jquery.min.js'
  };

  if ('local_search') {
    stellar.search = {};
    stellar.search.service = 'local_search';
    if (stellar.search.service == 'local_search') {
      let service_obj = Object.assign({}, {"field":"all","path":"/search.json","content":true,"sort":"-date"});
      stellar.search[stellar.search.service] = service_obj;
    }
  }

  // stellar js
  stellar.plugins.stellar = Object.assign({"sites":"/js/plugins/sites.js","friends":"/js/plugins/friends.js","ghinfo":"/js/plugins/ghinfo.js","timeline":"/js/plugins/timeline.js","linkcard":"/js/plugins/linkcard.js","fcircle":"/js/plugins/fcircle.js","weibo":"/js/plugins/weibo.js"});

  stellar.plugins.marked = Object.assign("https://cdn.bootcdn.net/ajax/libs/marked/4.0.18/marked.min.js");
  // optional plugins
  if ('true' == 'true') {
    stellar.plugins.lazyload = Object.assign({"enable":true,"js":"https://gcore.jsdelivr.net/npm/vanilla-lazyload@17.8.3/dist/lazyload.min.js","transition":"blur"});
  }
  if ('true' == 'true') {
    stellar.plugins.swiper = Object.assign({"enable":true,"css":"https://unpkg.com/swiper@8.4.5/swiper-bundle.min.css","js":"https://unpkg.com/swiper@8.4.5/swiper-bundle.min.js"});
  }
  if ('' == 'true') {
    stellar.plugins.scrollreveal = Object.assign({"enable":null,"js":"https://gcore.jsdelivr.net/npm/scrollreveal@4.0.9/dist/scrollreveal.min.js","distance":"8px","duration":500,"interval":100,"scale":1});
  }
  if ('true' == 'true') {
    stellar.plugins.preload = Object.assign({"enable":true,"service":"flying_pages","instant_page":"https://gcore.jsdelivr.net/gh/volantis-x/cdn-volantis@4.1.2/js/instant_page.js","flying_pages":"https://gcore.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.fancybox = Object.assign({"enable":true,"js":"https://gcore.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.umd.js","css":"https://gcore.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.css","selector":".swiper-slide img"});
  }
  if ('false' == 'true') {
    stellar.plugins.heti = Object.assign({"enable":false,"css":"https://unpkg.com/heti@0.9.2/umd/heti.min.css","js":"https://unpkg.com/heti@0.9.2/umd/heti-addon.min.js"});
  }
</script>

<!-- required -->

  
<script src="/js/main.js" async></script>



<!-- optional -->

  <script>
  function loadJS() {
    const els = document.querySelectorAll("#comments #giscus");
    if (els.length === 0) return;
    els.forEach((el, i) => {
      try {
        el.innerHTML = '';
      } catch (error) {
        console.log(error);
      }
      var script = document.createElement('script');
      script.src = 'https://giscus.app/client.js';
      script.async = true;
      for (let key of Object.keys(el.attributes)) {
        let attr = el.attributes[key];
        if (['class', 'id'].includes(attr.name) === false) {
          script.setAttribute(attr.name, attr.value);
        }
      }
      el.appendChild(script);
    });
  }
  window.addEventListener('DOMContentLoaded', (event) => {
    loadJS();
  });
</script>




<!-- inject -->


  </div>
  <!--动态线条背景-->
<script type="text/javascript"
color="220,220,220" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
</script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":-15,"vOffset":-15},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
